"""
ë°ì´í„° ìˆ˜ì§‘ ë° ìŠ¤ëƒ…ìƒ· ì €ì¥ ë¡œì§
"""
import os
import json
from datetime import datetime
from typing import List, Dict, Any

import youtube_api
import database
import senior_classifier
import view_score_calculator


def collect_trending_videos(
    category_ids: List[str],
    snapshot_date: str = None,
    max_results: int = 50
) -> Dict[str, Any]:
    """
    ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ ë° ì €ì¥

    Args:
        category_ids: ì¹´í…Œê³ ë¦¬ ID ë¦¬ìŠ¤íŠ¸
        snapshot_date: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        max_results: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜

    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
    """
    if snapshot_date is None:
        snapshot_date = datetime.now().strftime('%Y-%m-%d')

    # ê²°ê³¼ í†µê³„
    stats = {
        'snapshot_date': snapshot_date,
        'total_videos': 0,
        'new_videos': 0,
        'duplicate_skipped': 0,
        'categories': {}
    }

    # ìŠ¤ëƒ…ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    snapshot_dir = f'data/{snapshot_date}'
    os.makedirs(snapshot_dir, exist_ok=True)

    all_videos = []

    for category_id in category_ids:
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ {category_id} ìˆ˜ì§‘ ì¤‘...")

        # ì¸ê¸° ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
        videos = youtube_api.get_trending_videos(
            category_id=category_id,
            max_results=max_results
        )

        if not videos:
            print(f"âš ï¸  ì¹´í…Œê³ ë¦¬ {category_id}: ê²°ê³¼ ì—†ìŒ")
            stats['categories'][category_id] = {'collected': 0, 'new': 0, 'duplicates': 0}
            continue

        category_stats = {'collected': len(videos), 'new': 0, 'duplicates': 0}

        for video in videos:
            video_id = video['video_id']

            # ì¤‘ë³µ ì²´í¬: ê°™ì€ ë‚ ì§œ, ê°™ì€ ì¹´í…Œê³ ë¦¬ì— ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€
            if database.check_snapshot_exists(video_id, snapshot_date, category_id):
                print(f"  â­ï¸  ì¤‘ë³µ ìŠ¤í‚µ: {video['title'][:50]}")
                category_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 1. ë¹„ë””ì˜¤ ì •ë³´ ì €ì¥ (videos í…Œì´ë¸”)
            database.insert_video(video)

            # 2. ìŠ¤ëƒ…ìƒ· ì €ì¥ (snapshots í…Œì´ë¸”)
            snapshot_data = {
                'video_id': video_id,
                'category_id': category_id,
                'snapshot_date': snapshot_date,
                'view_count': video['view_count'],
                'like_count': video['like_count'],
                'comment_count': video['comment_count'],
                'rank_position': video['rank_position']
            }
            snapshot_id = database.insert_snapshot(snapshot_data)

            if snapshot_id is None:
                # ì¤‘ë³µ (ì´ë¯¸ ì¡´ì¬)
                category_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 3. ì±„ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë° ì €ì¥ (ViewScore ê³„ì‚°ì— í•„ìš”)
            channel_info_list = youtube_api.get_channel_info([video['channel_id']])
            channel_info = channel_info_list[0] if channel_info_list else None
            if channel_info:
                database.upsert_channel(channel_info)

            # 4. ViewScore ê³„ì‚° (NEW)
            view_score_result = view_score_calculator.calculate_view_score(
                video_data=video,
                snapshot_data=snapshot_data,
                channel_data=channel_info
            )
            view_score_result['snapshot_id'] = snapshot_id

            # 5. ViewScore ì €ì¥
            database.insert_view_score(view_score_result)

            # ìˆ˜ì§‘ ê²°ê³¼ì— ì¶”ê°€
            video['view_score'] = view_score_result
            all_videos.append(video)

            category_stats['new'] += 1
            stats['new_videos'] += 1

            print(f"  âœ“ {video['title'][:50]} (ViewScore: {view_score_result['score']:.1f})")

        stats['categories'][category_id] = category_stats
        stats['total_videos'] += category_stats['collected']

    # JSONL íŒŒì¼ë¡œ ì €ì¥ (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·)
    snapshot_file = f'{snapshot_dir}/videos.jsonl'
    with open(snapshot_file, 'a', encoding='utf-8') as f:
        for video in all_videos:
            f.write(json.dumps(video, ensure_ascii=False) + '\n')

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {snapshot_file}")
    print(f"   ì´ {stats['total_videos']}ê°œ, ì‹ ê·œ {stats['new_videos']}ê°œ, ì¤‘ë³µ ìŠ¤í‚µ {stats['duplicate_skipped']}ê°œ")

    return stats


def calculate_delta_views_for_date(snapshot_date: str, days: int = 14, data_source: str = 'all') -> List[Dict[str, Any]]:
    """
    íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë¹„ë””ì˜¤ì— ëŒ€í•´ Î”views ê³„ì‚°

    Args:
        snapshot_date: ê¸°ì¤€ ë‚ ì§œ
        days: ë¹„êµ ê¸°ê°„ (ê¸°ë³¸ 14ì¼)
        data_source: ë°ì´í„° ì†ŒìŠ¤ í•„í„° ('channel', 'category', 'all')

    Returns:
        Î”viewsê°€ í¬í•¨ëœ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
    """
    snapshots = database.get_snapshots_by_date_and_source(snapshot_date, data_source)

    results = []
    for snapshot in snapshots:
        video_id = snapshot['video_id']
        delta_views = database.get_delta_views(video_id, days)

        snapshot['delta_views_14d'] = delta_views if delta_views is not None else 0
        results.append(snapshot)

    return results




def load_snapshot_from_file(snapshot_date: str) -> List[Dict[str, Any]]:
    """
    íŒŒì¼ì—ì„œ ìŠ¤ëƒ…ìƒ· ë¶ˆëŸ¬ì˜¤ê¸° (API í˜¸ì¶œ ì—†ì´)

    Args:
        snapshot_date: ë‚ ì§œ (YYYY-MM-DD)

    Returns:
        ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
    """
    snapshot_file = f'data/{snapshot_date}/videos.jsonl'

    if not os.path.exists(snapshot_file):
        return []

    videos = []
    with open(snapshot_file, 'r', encoding='utf-8') as f:
        for line in f:
            videos.append(json.loads(line))

    return videos


def collect_from_channels(
    channel_ids: List[str],
    snapshot_date: str = None,
    max_results_per_channel: int = 50,
    days: int = 7,
    skip_today_collected: bool = False
) -> Dict[str, Any]:
    """
    ë“±ë¡ëœ ì±„ë„ë“¤ì˜ ìµœê·¼ ì˜ìƒ ìˆ˜ì§‘

    Args:
        channel_ids: ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
        snapshot_date: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        max_results_per_channel: ì±„ë„ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        days: ìµœê·¼ Nì¼ ì´ë‚´ ì˜ìƒ
        skip_today_collected: ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘í•œ ì±„ë„ ê±´ë„ˆë›°ê¸° (ì¿¼í„° ì ˆì•½)

    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
    """
    if snapshot_date is None:
        snapshot_date = datetime.now().strftime('%Y-%m-%d')

    # ê²°ê³¼ í†µê³„
    stats = {
        'snapshot_date': snapshot_date,
        'total_videos': 0,
        'new_videos': 0,
        'duplicate_skipped': 0,
        'channels_skipped': 0,
        'channels': {}
    }

    # ìŠ¤ëƒ…ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    snapshot_dir = f'data/{snapshot_date}'
    os.makedirs(snapshot_dir, exist_ok=True)

    all_videos = []

    for channel_id in channel_ids:
        # ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘í•œ ì±„ë„ì¸ì§€ í™•ì¸ (ì¿¼í„° ì ˆì•½)
        if skip_today_collected and database.check_channel_collected_today(channel_id):
            print(f"\nâ­ï¸  ì±„ë„ {channel_id} ìŠ¤í‚µ (ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘ ì™„ë£Œ)")
            stats['channels_skipped'] += 1
            stats['channels'][channel_id] = {'collected': 0, 'new': 0, 'duplicates': 0, 'skipped': True}
            continue

        print(f"\nğŸ“º ì±„ë„ {channel_id} ìˆ˜ì§‘ ì¤‘...")

        # ì±„ë„ì˜ ìµœê·¼ ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
        videos = youtube_api.get_channel_recent_videos(
            channel_id=channel_id,
            max_results=max_results_per_channel,
            days=days
        )

        if not videos:
            print(f"âš ï¸  ì±„ë„ {channel_id}: ê²°ê³¼ ì—†ìŒ")
            stats['channels'][channel_id] = {'collected': 0, 'new': 0, 'duplicates': 0}
            continue

        channel_stats = {'collected': len(videos), 'new': 0, 'duplicates': 0}

        for video in videos:
            video_id = video['video_id']

            # ì¤‘ë³µ ì²´í¬: ê°™ì€ ë‚ ì§œì— ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€
            # ì±„ë„ ê¸°ë°˜ ìˆ˜ì§‘ì€ category_idë¥¼ 'channel'ë¡œ í‘œì‹œ
            if database.check_snapshot_exists(video_id, snapshot_date, f'channel:{channel_id}'):
                print(f"  â­ï¸  ì¤‘ë³µ ìŠ¤í‚µ: {video['title'][:50]}")
                channel_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 1. ë¹„ë””ì˜¤ ì •ë³´ ì €ì¥ (videos í…Œì´ë¸”)
            database.insert_video(video)

            # 2. ìŠ¤ëƒ…ìƒ· ì €ì¥ (snapshots í…Œì´ë¸”)
            snapshot_data = {
                'video_id': video_id,
                'category_id': f'channel:{channel_id}',  # ì±„ë„ ê¸°ë°˜ì„ì„ í‘œì‹œ
                'snapshot_date': snapshot_date,
                'view_count': video['view_count'],
                'like_count': video['like_count'],
                'comment_count': video['comment_count'],
                'rank_position': video['rank_position']
            }
            snapshot_id = database.insert_snapshot(snapshot_data)

            if snapshot_id is None:
                # ì¤‘ë³µ (ì´ë¯¸ ì¡´ì¬)
                channel_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 3. ì±„ë„ ì •ë³´ ì¡°íšŒ (ViewScore ê³„ì‚°ì— í•„ìš”)
            # ì´ë¯¸ channels í…Œì´ë¸”ì— ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ DBì—ì„œ ë¨¼ì € ì¡°íšŒ
            channel_info = database.get_channel_by_id(channel_id)
            if not channel_info:
                # DBì— ì—†ìœ¼ë©´ API í˜¸ì¶œ
                channel_info_list = youtube_api.get_channel_info([channel_id])
                channel_info = channel_info_list[0] if channel_info_list else None
                if channel_info:
                    database.upsert_channel(channel_info)

            # 4. ViewScore ê³„ì‚° (NEW)
            view_score_result = view_score_calculator.calculate_view_score(
                video_data=video,
                snapshot_data=snapshot_data,
                channel_data=channel_info
            )
            view_score_result['snapshot_id'] = snapshot_id

            # 5. ViewScore ì €ì¥
            database.insert_view_score(view_score_result)

            # ìˆ˜ì§‘ ê²°ê³¼ì— ì¶”ê°€
            video['view_score'] = view_score_result
            all_videos.append(video)

            channel_stats['new'] += 1
            stats['new_videos'] += 1

            print(f"  âœ“ {video['title'][:50]} (ViewScore: {view_score_result['score']:.1f})")

        stats['channels'][channel_id] = channel_stats
        stats['total_videos'] += channel_stats['collected']

        # ì±„ë„ ìˆ˜ì§‘ ë‚ ì§œ ì—…ë°ì´íŠ¸ (ì˜¤ëŠ˜ë¡œ ê°±ì‹ )
        database.update_channel_collected_date(channel_id)

    # JSONL íŒŒì¼ë¡œ ì €ì¥ (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·)
    snapshot_file = f'{snapshot_dir}/videos_channels.jsonl'
    with open(snapshot_file, 'a', encoding='utf-8') as f:
        for video in all_videos:
            f.write(json.dumps(video, ensure_ascii=False) + '\n')

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {snapshot_file}")
    print(f"   ì´ {stats['total_videos']}ê°œ, ì‹ ê·œ {stats['new_videos']}ê°œ, ì¤‘ë³µ ìŠ¤í‚µ {stats['duplicate_skipped']}ê°œ")

    return stats


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸: ì¹´í…Œê³ ë¦¬ 10 (Music) ìˆ˜ì§‘
    print("=== ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    database.init_database()

    stats = collect_trending_videos(
        category_ids=['10'],
        max_results=10
    )

    print("\n=== ìˆ˜ì§‘ í†µê³„ ===")
    print(json.dumps(stats, indent=2, ensure_ascii=False))
