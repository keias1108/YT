"""
ë°ì´í„° ìˆ˜ì§‘ ë° ìŠ¤ëƒ…ìƒ· ì €ì¥ ë¡œì§
"""
import os
import json
from datetime import datetime
from typing import List, Dict, Any

import youtube_api
import database
import senior_classifier


def collect_trending_videos(
    category_ids: List[str],
    snapshot_date: str = None,
    max_results: int = 50
) -> Dict[str, Any]:
    """
    ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ ë° ì €ì¥

    Args:
        category_ids: ì¹´í…Œê³ ë¦¬ ID ë¦¬ìŠ¤íŠ¸
        snapshot_date: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        max_results: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜

    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
    """
    if snapshot_date is None:
        snapshot_date = datetime.now().strftime('%Y-%m-%d')

    # ê²°ê³¼ í†µê³„
    stats = {
        'snapshot_date': snapshot_date,
        'total_videos': 0,
        'new_videos': 0,
        'duplicate_skipped': 0,
        'categories': {}
    }

    # ìŠ¤ëƒ…ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    snapshot_dir = f'data/{snapshot_date}'
    os.makedirs(snapshot_dir, exist_ok=True)

    all_videos = []

    for category_id in category_ids:
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ {category_id} ìˆ˜ì§‘ ì¤‘...")

        # ì¸ê¸° ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
        videos = youtube_api.get_trending_videos(
            category_id=category_id,
            max_results=max_results
        )

        if not videos:
            print(f"âš ï¸  ì¹´í…Œê³ ë¦¬ {category_id}: ê²°ê³¼ ì—†ìŒ")
            stats['categories'][category_id] = {'collected': 0, 'new': 0, 'duplicates': 0}
            continue

        category_stats = {'collected': len(videos), 'new': 0, 'duplicates': 0}

        for video in videos:
            video_id = video['video_id']

            # ì¤‘ë³µ ì²´í¬: ê°™ì€ ë‚ ì§œ, ê°™ì€ ì¹´í…Œê³ ë¦¬ì— ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€
            if database.check_snapshot_exists(video_id, snapshot_date, category_id):
                print(f"  â­ï¸  ì¤‘ë³µ ìŠ¤í‚µ: {video['title'][:50]}")
                category_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 1. ë¹„ë””ì˜¤ ì •ë³´ ì €ì¥ (videos í…Œì´ë¸”)
            database.insert_video(video)

            # 2. ìŠ¤ëƒ…ìƒ· ì €ì¥ (snapshots í…Œì´ë¸”)
            snapshot_data = {
                'video_id': video_id,
                'category_id': category_id,
                'snapshot_date': snapshot_date,
                'view_count': video['view_count'],
                'like_count': video['like_count'],
                'comment_count': video['comment_count'],
                'rank_position': video['rank_position']
            }
            snapshot_id = database.insert_snapshot(snapshot_data)

            if snapshot_id is None:
                # ì¤‘ë³µ (ì´ë¯¸ ì¡´ì¬)
                category_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 3. SeniorScore ê³„ì‚°
            # (ëŒ“ê¸€ì€ API ì¿¼í„° ì ˆì•½ì„ ìœ„í•´ ì„ íƒì ìœ¼ë¡œ)
            # comments = youtube_api.get_video_comments(video_id, max_results=50)
            comments = None  # ì¼ë‹¨ ëŒ“ê¸€ ì—†ì´

            senior_score_result = senior_classifier.calculate_senior_score(video, comments)
            senior_score_result['snapshot_id'] = snapshot_id

            # 4. SeniorScore ì €ì¥
            database.insert_senior_score(senior_score_result)

            # 5. ì±„ë„ ì •ë³´ ì—…ë°ì´íŠ¸ (ì„ íƒì )
            # channel_info = youtube_api.get_channel_info([video['channel_id']])
            # if channel_info:
            #     database.upsert_channel(channel_info[0])

            # ìˆ˜ì§‘ ê²°ê³¼ì— ì¶”ê°€
            video['senior_score'] = senior_score_result
            all_videos.append(video)

            category_stats['new'] += 1
            stats['new_videos'] += 1

            print(f"  âœ“ {video['title'][:50]} (SeniorScore: {senior_score_result['score']})")

        stats['categories'][category_id] = category_stats
        stats['total_videos'] += category_stats['collected']

    # JSONL íŒŒì¼ë¡œ ì €ì¥ (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·)
    snapshot_file = f'{snapshot_dir}/videos.jsonl'
    with open(snapshot_file, 'a', encoding='utf-8') as f:
        for video in all_videos:
            f.write(json.dumps(video, ensure_ascii=False) + '\n')

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {snapshot_file}")
    print(f"   ì´ {stats['total_videos']}ê°œ, ì‹ ê·œ {stats['new_videos']}ê°œ, ì¤‘ë³µ ìŠ¤í‚µ {stats['duplicate_skipped']}ê°œ")

    return stats


def calculate_delta_views_for_date(snapshot_date: str, days: int = 14) -> List[Dict[str, Any]]:
    """
    íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë¹„ë””ì˜¤ì— ëŒ€í•´ Î”views ê³„ì‚°

    Args:
        snapshot_date: ê¸°ì¤€ ë‚ ì§œ
        days: ë¹„êµ ê¸°ê°„ (ê¸°ë³¸ 14ì¼)

    Returns:
        Î”viewsê°€ í¬í•¨ëœ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
    """
    snapshots = database.get_snapshots_by_date(snapshot_date)

    results = []
    for snapshot in snapshots:
        video_id = snapshot['video_id']
        delta_views = database.get_delta_views(video_id, days)

        snapshot['delta_views_14d'] = delta_views if delta_views is not None else 0
        results.append(snapshot)

    return results


def get_ranked_senior_videos(
    snapshot_date: str = None,
    senior_threshold: float = 5.0,
    min_delta_views: int = 0,
    limit: int = 50
) -> List[Dict[str, Any]]:
    """
    ì‹œë‹ˆì–´ ì¹œí™” ì˜ìƒ ë­í‚¹ ê°€ì ¸ì˜¤ê¸°

    Args:
        snapshot_date: ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
        senior_threshold: SeniorScore ì„ê³„ê°’
        min_delta_views: ìµœì†Œ Î”views (0ì´ë©´ í•„í„°ë§ ì•ˆí•¨)
        limit: ìµœëŒ€ ë°˜í™˜ ìˆ˜

    Returns:
        ë­í‚¹ ë¦¬ìŠ¤íŠ¸ (SeniorScore ë‚´ë¦¼ì°¨ìˆœ)
    """
    if snapshot_date is None:
        snapshot_date = datetime.now().strftime('%Y-%m-%d')

    # Î”views ê³„ì‚°
    videos = calculate_delta_views_for_date(snapshot_date)

    # í•„í„°ë§
    filtered = []
    for video in videos:
        senior_score = video.get('senior_score', 0)
        delta_views = video.get('delta_views_14d', 0)

        if senior_score >= senior_threshold and delta_views >= min_delta_views:
            filtered.append(video)

    # ì •ë ¬: SeniorScore ë‚´ë¦¼ì°¨ìˆœ, ë™ì ì´ë©´ Î”views ë‚´ë¦¼ì°¨ìˆœ
    filtered.sort(key=lambda x: (x.get('senior_score', 0), x.get('delta_views_14d', 0)), reverse=True)

    return filtered[:limit]


def load_snapshot_from_file(snapshot_date: str) -> List[Dict[str, Any]]:
    """
    íŒŒì¼ì—ì„œ ìŠ¤ëƒ…ìƒ· ë¶ˆëŸ¬ì˜¤ê¸° (API í˜¸ì¶œ ì—†ì´)

    Args:
        snapshot_date: ë‚ ì§œ (YYYY-MM-DD)

    Returns:
        ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
    """
    snapshot_file = f'data/{snapshot_date}/videos.jsonl'

    if not os.path.exists(snapshot_file):
        return []

    videos = []
    with open(snapshot_file, 'r', encoding='utf-8') as f:
        for line in f:
            videos.append(json.loads(line))

    return videos


def collect_from_channels(
    channel_ids: List[str],
    snapshot_date: str = None,
    max_results_per_channel: int = 50,
    days: int = 7,
    skip_today_collected: bool = False
) -> Dict[str, Any]:
    """
    ë“±ë¡ëœ ì±„ë„ë“¤ì˜ ìµœê·¼ ì˜ìƒ ìˆ˜ì§‘

    Args:
        channel_ids: ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
        snapshot_date: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        max_results_per_channel: ì±„ë„ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜
        days: ìµœê·¼ Nì¼ ì´ë‚´ ì˜ìƒ
        skip_today_collected: ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘í•œ ì±„ë„ ê±´ë„ˆë›°ê¸° (ì¿¼í„° ì ˆì•½)

    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
    """
    if snapshot_date is None:
        snapshot_date = datetime.now().strftime('%Y-%m-%d')

    # ê²°ê³¼ í†µê³„
    stats = {
        'snapshot_date': snapshot_date,
        'total_videos': 0,
        'new_videos': 0,
        'duplicate_skipped': 0,
        'channels_skipped': 0,
        'channels': {}
    }

    # ìŠ¤ëƒ…ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    snapshot_dir = f'data/{snapshot_date}'
    os.makedirs(snapshot_dir, exist_ok=True)

    all_videos = []

    for channel_id in channel_ids:
        # ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘í•œ ì±„ë„ì¸ì§€ í™•ì¸ (ì¿¼í„° ì ˆì•½)
        if skip_today_collected and database.check_channel_collected_today(channel_id):
            print(f"\nâ­ï¸  ì±„ë„ {channel_id} ìŠ¤í‚µ (ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘ ì™„ë£Œ)")
            stats['channels_skipped'] += 1
            stats['channels'][channel_id] = {'collected': 0, 'new': 0, 'duplicates': 0, 'skipped': True}
            continue

        print(f"\nğŸ“º ì±„ë„ {channel_id} ìˆ˜ì§‘ ì¤‘...")

        # ì±„ë„ì˜ ìµœê·¼ ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
        videos = youtube_api.get_channel_recent_videos(
            channel_id=channel_id,
            max_results=max_results_per_channel,
            days=days
        )

        if not videos:
            print(f"âš ï¸  ì±„ë„ {channel_id}: ê²°ê³¼ ì—†ìŒ")
            stats['channels'][channel_id] = {'collected': 0, 'new': 0, 'duplicates': 0}
            continue

        channel_stats = {'collected': len(videos), 'new': 0, 'duplicates': 0}

        for video in videos:
            video_id = video['video_id']

            # ì¤‘ë³µ ì²´í¬: ê°™ì€ ë‚ ì§œì— ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€
            # ì±„ë„ ê¸°ë°˜ ìˆ˜ì§‘ì€ category_idë¥¼ 'channel'ë¡œ í‘œì‹œ
            if database.check_snapshot_exists(video_id, snapshot_date, f'channel:{channel_id}'):
                print(f"  â­ï¸  ì¤‘ë³µ ìŠ¤í‚µ: {video['title'][:50]}")
                channel_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 1. ë¹„ë””ì˜¤ ì •ë³´ ì €ì¥ (videos í…Œì´ë¸”)
            database.insert_video(video)

            # 2. ìŠ¤ëƒ…ìƒ· ì €ì¥ (snapshots í…Œì´ë¸”)
            snapshot_data = {
                'video_id': video_id,
                'category_id': f'channel:{channel_id}',  # ì±„ë„ ê¸°ë°˜ì„ì„ í‘œì‹œ
                'snapshot_date': snapshot_date,
                'view_count': video['view_count'],
                'like_count': video['like_count'],
                'comment_count': video['comment_count'],
                'rank_position': video['rank_position']
            }
            snapshot_id = database.insert_snapshot(snapshot_data)

            if snapshot_id is None:
                # ì¤‘ë³µ (ì´ë¯¸ ì¡´ì¬)
                channel_stats['duplicates'] += 1
                stats['duplicate_skipped'] += 1
                continue

            # 3. SeniorScore ê³„ì‚°
            comments = None  # API ì¿¼í„° ì ˆì•½
            senior_score_result = senior_classifier.calculate_senior_score(video, comments)
            senior_score_result['snapshot_id'] = snapshot_id

            # 4. SeniorScore ì €ì¥
            database.insert_senior_score(senior_score_result)

            # ìˆ˜ì§‘ ê²°ê³¼ì— ì¶”ê°€
            video['senior_score'] = senior_score_result
            all_videos.append(video)

            channel_stats['new'] += 1
            stats['new_videos'] += 1

            print(f"  âœ“ {video['title'][:50]} (SeniorScore: {senior_score_result['score']})")

        stats['channels'][channel_id] = channel_stats
        stats['total_videos'] += channel_stats['collected']

        # ì±„ë„ ìˆ˜ì§‘ ë‚ ì§œ ì—…ë°ì´íŠ¸ (ì˜¤ëŠ˜ë¡œ ê°±ì‹ )
        database.update_channel_collected_date(channel_id)

    # JSONL íŒŒì¼ë¡œ ì €ì¥ (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·)
    snapshot_file = f'{snapshot_dir}/videos_channels.jsonl'
    with open(snapshot_file, 'a', encoding='utf-8') as f:
        for video in all_videos:
            f.write(json.dumps(video, ensure_ascii=False) + '\n')

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {snapshot_file}")
    print(f"   ì´ {stats['total_videos']}ê°œ, ì‹ ê·œ {stats['new_videos']}ê°œ, ì¤‘ë³µ ìŠ¤í‚µ {stats['duplicate_skipped']}ê°œ")

    return stats


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸: ì¹´í…Œê³ ë¦¬ 10 (Music) ìˆ˜ì§‘
    print("=== ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    database.init_database()

    stats = collect_trending_videos(
        category_ids=['10'],
        max_results=10
    )

    print("\n=== ìˆ˜ì§‘ í†µê³„ ===")
    print(json.dumps(stats, indent=2, ensure_ascii=False))
